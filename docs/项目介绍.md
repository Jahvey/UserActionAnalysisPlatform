###Spark电商用户行为分析
####模块一：用户访问session分析
> 用户访问session介绍:
  用户在电商网站上,通常会有很多的点击行为,通常都是进入首页;然后可能点击首页上的一些商品;点击首页上的一些品类;也可能随时在搜索框里面搜索关键词;还可能将一些商品加入购物车;对购物车中的多个商品下订单;最后对订单中的多个商品进行支付.
  用户的每一次操作,其实可以理解为一个action,比如点击,搜索,下单,支付.
  用户session,指的就是:从用户第一次进入首页,session就开始了,然后在一定时间范围内,直到最后操作完(可能做了几十次、甚至上百次操作).离开网站,关闭浏览器,或者长时间没有做操作,那么session就结束了.
  以上用户在网站内的访问过程,就称之为一次session.**简单理解,session就是某一天某一个时间段内,某个用户对网站从打开/进入,到做了大量操作,到最后关闭浏览器的过程,就叫做session.**    
  session实际上就是一个电商网站中最基本的数据和大数据.那么大数据,面向C端,也就是customer,消费者,用户端的,分析,基本是最基本的就是面向用户访问行为/用户访问session.

> 在实际企业项目中的使用架构:
  1.J2EE的平台,通过这个J2EE平台可以让使用者,提交各种各样的分析任务,其中就包括一个模块就是用户访问session分析模块;可以指定各种各样的筛选条件,比如年龄范围、职业、城市等等.
  2.J2EE平台接收到了执行统计分析任务的请求之后,会调用底层的封装了spark-submit的shell脚本(Runtime、Process),shell脚本进而提交我们编写的Spark作业.
  3.Spark作业获取使用者指定的筛选参数,然后运行复杂的作业逻辑,进行该模块的统计和分析.
  4.Spark作业统计和分析的结果,会写入MySQL中指定的表.
  5.最后,J2EE平台使用者可以通过前端页面,以表格、图表的形式展示和查看MySQL中存储的该统计分析任务的结果数据.
  
模块的目标：对用户访问session进行分析
1、可以根据使用者指定的某些条件，筛选出指定的一些用户（有特定年龄、职业、城市）；
2、对这些用户在指定日期范围内发起的session，进行聚合统计，比如，统计出访问时长在0~3s的session占总session数量的比例；
3、按时间比例，比如一天有24个小时，其中12:00~13:00的session数量占当天总session数量的50%，当天总session数量是10000个，那么当天总共要抽取1000个session，ok，12:00~13:00的用户，就得抽取1000*50%=500。而且这500个需要随机抽取。
4、获取点击量、下单量和支付量都排名10的商品种类
5、获取top10的商品种类的点击数量排名前10的session
6、开发完毕了以上功能之后，需要进行大量、复杂、高端、全套的性能调优
7、十亿级数据量的troubleshooting（故障解决）的经验总结
8、数据倾斜的完美解决方案
9、使用mock（模拟）的数据，对模块进行调试、运行和演示效果

#####数据表
表名：user_visit_action（Hive表）
date：日期，代表这个用户点击行为是在哪一天发生的
user_id：代表这个点击行为是哪一个用户执行的
session_id ：唯一标识了某个用户的一个访问session
page_id ：点击了某些商品/品类，也可能是搜索了某个关键词，然后进入了某个页面，页面的id
action_time ：这个点击行为发生的时间点
search_keyword ：如果用户执行的是一个搜索行为，比如说在网站/app中，搜索了某个关键词，然后会跳转到商品列表页面；搜索的关键词
click_category_id ：可能是在网站首页，点击了某个品类（美食、电子设备、电脑）
click_product_id ：可能是在网站首页，或者是在商品列表页，点击了某个商品（比如呷哺呷哺火锅XX路店3人套餐、iphone 6s）
order_category_ids ：代表了可能将某些商品加入了购物车，然后一次性对购物车中的商品下了一个订单，这就代表了某次下单的行为中，有哪些
商品品类，可能有6个商品，但是就对应了2个品类，比如有3根火腿肠（食品品类），3个电池（日用品品类）
order_product_ids ：某次下单，具体对哪些商品下的订单
pay_category_ids ：代表的是，对某个订单，或者某几个订单，进行了一次支付的行为，对应了哪些品类
pay_product_ids：代表的，支付行为下，对应的哪些具体的商品

**user_visit_action表，其实就是放，比如说网站，或者是app，每天的点击流的数据。可以理解为，用户对网站/app每点击一下，就会代表在这个表里面的一条数据。**

表名：user_info（Hive表）
user_id：其实就是每一个用户的唯一标识，通常是自增长的Long类型，BigInt类型
username：是每个用户的登录名
name：每个用户自己的昵称、或者是真实姓名
age：用户的年龄
professional：用户的职业
city：用户所在的城市

表名：task（MySQL表）
task_id：表的主键
task_name：任务名称
create_time：创建时间
start_time：开始运行的时间
finish_time：结束运行的时间
task_type：任务类型，就是说，在一套大数据平台中，肯定会有各种不同类型的统计分析任务，比如说用户访问session分析任务，页面单跳转化率统计任务；所以这个字段就标识了每个任务的类型
task_status：任务状态，任务对应的就是一次Spark作业的运行，这里就标识了，Spark作业是新建，还没运行，还是正在运行，还是已经运行完毕
task_param：最最重要，用来使用JSON的格式，来封装用户提交的任务对应的特殊的筛选参数

task表，其实是用来保存平台的使用者，通过J2EE系统，提交的基于特定筛选参数的分析任务，的信息，就会通过J2EE系统保存到task表中来。之所以使用MySQL表，是因为J2EE系统是要实现快速的实时插入和查询的。